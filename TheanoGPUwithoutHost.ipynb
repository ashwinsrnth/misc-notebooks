{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Theano GPU without host transfers"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the following experiment, I compare different approaches to computing a simple tensor expression using the GPU with Theano.\n",
      "The focus is on modularity: we don't want to have to resort to using `shared` to get optimal results."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from theano import shared, function, tensor, sandbox\n",
      "import numpy as np\n",
      "import pygpu"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "Using device cuda0: Tesla K20m\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# create a GPU type (this is how it's done in the theano unit tests):\n",
      "GpuTensor3 = sandbox.gpuarray.type.GpuArrayType('float64', broadcastable=(False, False, False))\n",
      "x = GpuTensor3()\n",
      "\n",
      "# compile the function:\n",
      "f = function([x], (x*x.T*x).T)\n",
      "print f.maker.fgraph.toposort()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[InplaceGpuDimShuffle{2,1,0}(<GpuArray<float64>>), GpuElemwise{mul,no_inplace}(InplaceGpuDimShuffle{2,1,0}.0, InplaceGpuDimShuffle{2,1,0}.0, <GpuArray<float64>>), HostFromGpu(gpuarray)(GpuElemwise{mul,no_inplace}.0)]\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# create arrays on the host and the device\n",
      "y = np.random.random([100, 100, 100])\n",
      "y_gpu = pygpu.array(y)\n",
      "print type(y_gpu)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<type 'pygpu.gpuarray.GpuArray'>\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%timeit f(y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "100 loops, best of 3: 7.81 ms per loop\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%timeit f(y_gpu)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "100 loops, best of 3: 3.98 ms per loop\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# now, don't copy back to host:\n",
      "f = function([x], sandbox.gpuarray.basic_ops.gpu_from_host((x*x.T*x).T))\n",
      "print f.maker.fgraph.toposort()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[InplaceGpuDimShuffle{2,1,0}(<GpuArray<float64>>), GpuElemwise{mul,no_inplace}(InplaceGpuDimShuffle{2,1,0}.0, InplaceGpuDimShuffle{2,1,0}.0, <GpuArray<float64>>)]\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%timeit f(y_gpu)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1000 loops, best of 3: 1.11 ms per loop\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Compare with shared"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The advantage of shared is that there's no awkward definition of types, but you lose some modularity. It also maintains state."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y_shared = shared(y)\n",
      "f = function([], sandbox.gpuarray.basic_ops.gpu_from_host((y_shared*y_shared.T*y_shared).T))\n",
      "print f.maker.fgraph.toposort"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<bound method FunctionGraph.toposort of [GpuElemwise{mul,no_inplace}(*1 -> InplaceGpuDimShuffle{2,1,0}(<GpuArray<float64>>), *1, <GpuArray<float64>>)]>\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import time\n",
      "t1 = time.time()\n",
      "for i in range(5):\n",
      "    f()\n",
      "t2 = time.time()\n",
      "print (t2-t1)*1000/5.0, ' ms'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1.18141174316  ms\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Compare with NumPy"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%timeit (y*y.T*y).T"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "100 loops, best of 3: 11.5 ms per loop\n"
       ]
      }
     ],
     "prompt_number": 10
    }
   ],
   "metadata": {}
  }
 ]
}